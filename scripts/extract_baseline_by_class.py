import os
import cv2
import torch
import numpy as np
from ultralytics import YOLO
from tqdm import tqdm

# ===================== é…ç½®åŒº =====================
DATASET_ROOT = r"D:/github/DriftKit/dataset"   # âš ï¸ ç”¨ raw string
MODEL_PATH = "yolo11n.pt"
IMG_SIZE = 640
DEVICE = "cpu"   # ä½ ç›®å‰æ˜¯ CPUï¼Œåé¢å¯æ”¹ cuda
OUTPUT_DIR = "outputs"
HOOK_LAYER_INDEX = 10   # C2PSAï¼ˆé«˜è¯­ä¹‰ backbone æœ«ç«¯ï¼‰
# ==================================================

os.makedirs(OUTPUT_DIR, exist_ok=True)

# ---------------- åŠ è½½æ¨¡å‹ ----------------
model = YOLO(MODEL_PATH)
net = model.model
net.to(DEVICE)
net.eval()

# ---------------- Hook å®¹å™¨ ----------------
feature_buffer = []

def hook_fn(module, inp, out):
    """
    out: [B, C, H, W]
    -> GAP -> [B, C]
    """
    with torch.no_grad():
        feat = out.mean(dim=[2, 3])  # Global Average Pooling
        feature_buffer.append(feat.cpu())

# æ³¨å†Œ hook
hook_handle = net.model[HOOK_LAYER_INDEX].register_forward_hook(hook_fn)

# ---------------- å›¾åƒé¢„å¤„ç† ----------------
def load_image(img_path):
    img = cv2.imread(img_path)
    if img is None:
        return None
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
    img = img.astype(np.float32) / 255.0
    img = torch.from_numpy(img).permute(2, 0, 1).unsqueeze(0)
    return img.to(DEVICE)

# ---------------- ä¸»é€»è¾‘ ----------------
all_features = {}
all_filenames = {}  # æ–°å¢ï¼šä¿å­˜çœŸå®æ–‡ä»¶å
total_images = 0

for split in ["train", "val"]:
    split_dir = os.path.join(DATASET_ROOT, split)
    print(f"\n== SCANNING {split_dir} ==")

    if not os.path.isdir(split_dir):
        print(f"âŒ Not found: {split_dir}")
        continue

    for cls in sorted(os.listdir(split_dir)):
        cls_dir = os.path.join(split_dir, cls)
        if not os.path.isdir(cls_dir):
            continue

        print(f"  -> Class {cls}")
        cls_features = []
        cls_filenames = []  # æ–°å¢

        img_files = sorted(os.listdir(cls_dir))
        for f in tqdm(img_files, desc=f"{split}/{cls}", leave=False):
            if not f.lower().endswith((".jpg", ".jpeg", ".png", ".bmp")):
                continue

            img_path = os.path.join(cls_dir, f)
            img = load_image(img_path)
            if img is None:
                continue

            feature_buffer.clear()
            with torch.no_grad():
                _ = net(img)

            if len(feature_buffer) == 0:
                print(f"âš ï¸ No feature captured for {img_path}")
                continue

            feat = feature_buffer[0].squeeze(0)  # [256]
            cls_features.append(feat.numpy())
            cls_filenames.append(f)  # ä¿å­˜çœŸå®æ–‡ä»¶å
            total_images += 1

        if len(cls_features) > 0:
            cls_features = np.stack(cls_features).astype(np.float32)
            all_features[f"{split}_{cls}"] = cls_features
            all_filenames[f"{split}_{cls}"] = cls_filenames  # ä¿å­˜å¯¹åº”æ–‡ä»¶å
            print(f"    âœ” Collected {cls_features.shape[0]} features")

print(f"\nâœ… TOTAL IMAGES PROCESSED: {total_images}")

# ---------------- ä¿å­˜ç‰¹å¾ ----------------
for k, v in all_features.items():
    out_path = os.path.join(OUTPUT_DIR, f"{k}_features.npy")
    np.save(out_path, v)
    print(f"Saved: {out_path}, shape={v.shape}")

# ---------------- ä¿å­˜å¯¹åº”æ–‡ä»¶å ----------------
for k, v in all_filenames.items():
    out_path = os.path.join(OUTPUT_DIR, f"{k}_filenames.npy")
    np.save(out_path, np.array(v))  # ä¿å­˜å­—ç¬¦ä¸²æ•°ç»„
    print(f"Saved filenames: {out_path}, length={len(v)}")

# ---------------- æ¸…ç† ----------------
hook_handle.remove()

print("\nğŸ¯ Baseline feature extraction DONE.")

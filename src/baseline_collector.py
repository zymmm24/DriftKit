import torch
import numpy as np
import pandas as pd
import pickle
import gc
from ultralytics import YOLO
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from pathlib import Path


class YOLO11AutoCollector:
    def __init__(self, model_path, dataset_root="dataset"):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸ“¡ è¿è¡Œè®¾å¤‡: {self.device}")

        self.model = YOLO(model_path)
        self.dataset_root = Path(dataset_root)
        self.label_map = self.model.names

        # ===== æ ¸å¿ƒä¿®æ”¹ï¼šé”å®šåˆ†ç±»å¤´å‰ä¸€å±‚ =====
        self.target_layer_idx = self._lock_feature_layer()

        self._current_batch_features = []
        self._hook_handle = None
        self._register_hook()

    def _lock_feature_layer(self):
        """
        YOLO classify æ¨¡å‹ä¸­ï¼Œå€’æ•°ç¬¬äºŒå±‚æ˜¯ç¨³å®šçš„è¯­ä¹‰ç‰¹å¾å±‚
        """
        layers = list(self.model.model.model)
        idx = len(layers) - 2
        print(f"ğŸ¯ é”å®šç‰¹å¾å±‚: ç´¢å¼• [{idx}], ç±»å‹ [{layers[idx].__class__.__name__}]")
        return idx

    def _hook_fn(self, module, input, output):
        """
        ç¨³å®šçš„ç‰¹å¾æŠ“å–å‡½æ•°
        """
        if isinstance(output, (list, tuple)):
            output = output[0]

        feat = output.detach().cpu()

        # åˆ†ç±»æ¨¡å‹é€šå¸¸æ˜¯ [B, C] æˆ– [B, C, 1, 1]
        if feat.dim() == 4:
            feat = torch.mean(feat, dim=[2, 3])

        self._current_batch_features.extend(feat.numpy())

    def _register_hook(self):
        layers = list(self.model.model.model)
        layer = layers[self.target_layer_idx]
        self._hook_handle = layer.register_forward_hook(self._hook_fn)

    def run(self):
        if not self.dataset_root.exists():
            print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {self.dataset_root}")
            return None

        img_list = [
            str(p) for p in self.dataset_root.rglob("*")
            if p.suffix.lower() in [".jpg", ".png", ".jpeg"]
        ]

        if not img_list:
            print(f"âš ï¸ åœ¨ {self.dataset_root} ä¸­æœªæ‰¾åˆ°å›¾ç‰‡ã€‚")
            return None

        print(f"ğŸš€ å¼€å§‹å¤„ç† {self.dataset_root.name}ï¼Œå…± {len(img_list)} å¼ å›¾ç‰‡...")

        all_records = []

        results = self.model.predict(
            source=img_list,
            batch=1,              # ä¿è¯é¡ºåºä¸€è‡´
            imgsz=224,
            stream=True,
            device=self.device,
            verbose=False
        )

        for res in results:
            if not self._current_batch_features:
                print(f"âš ï¸ æœªæŠ“å–åˆ°ç‰¹å¾: {res.path}")
                continue

            img_emb = self._current_batch_features.pop(0)

            record = {
                "img_name": Path(res.path).name,
                "image_embedding": img_emb.astype(np.float32),
                "label": "unknown",
                "conf": 0.0
            }

            # åˆ†ç±»è¾“å‡º
            if hasattr(res, 'probs') and res.probs is not None:
                cls_id = int(res.probs.top1)
                record["label"] = self.label_map.get(cls_id, f"class_{cls_id}")
                record["conf"] = float(res.probs.top1conf)

            all_records.append(record)

            if len(all_records) % 100 == 0:
                print(f"å·²å¤„ç†: {len(all_records)}")
                gc.collect()

        if self._hook_handle:
            self._hook_handle.remove()

        df = pd.DataFrame(all_records)
        print(f"âœ… ç‰¹å¾æå–å®Œæˆ: {df.shape}")
        return df

    def save_assets(self, df, folder="baseline_assets"):
        if df is None or df.empty:
            print("âŒ æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç”ŸæˆåŸºå‡†èµ„äº§ã€‚")
            return

        Path(folder).mkdir(parents=True, exist_ok=True)

        X = np.stack(df['image_embedding'].values)
        scaler = StandardScaler()

        n_comp = min(128, X.shape[0], X.shape[1])
        pca = PCA(n_components=n_comp)

        X_pca = pca.fit_transform(scaler.fit_transform(X))
        df['embedding_pca'] = list(X_pca.astype(np.float16))

        df.drop(columns=['image_embedding']).to_pickle(
            f"{folder}/baseline_db.pkl"
        )

        with open(f"{folder}/pca_scaler.pkl", "wb") as f:
            pickle.dump(
                {"scaler": scaler, "pca": pca, "names": self.label_map}, f
            )

        print(f"ğŸ“¦ åŸºå‡†èµ„äº§å·²ä¿å­˜è‡³: {folder}")


if __name__ == "__main__":
    MODEL_P = "runs/classify/train2/weights/best.pt"
    TRAIN_D = "D:/github/DriftKit/dataset/train"
    VAL_D = "D:/github/DriftKit/dataset/val"

    # Step 1: Train -> Baseline
    print("\n[STEP 1] ç”Ÿæˆ Baseline")
    coll_train = YOLO11AutoCollector(MODEL_P, TRAIN_D)
    df_train = coll_train.run()
    if df_train is not None:
        coll_train.save_assets(df_train)

    # Step 2: Val -> Test set
    pca_path = Path("../baseline_assets/pca_scaler.pkl")
    if pca_path.exists():
        print("\n[STEP 2] ç”Ÿæˆ Val æµ‹è¯•æ•°æ®")
        coll_val = YOLO11AutoCollector(MODEL_P, VAL_D)
        df_val = coll_val.run()

        if df_val is not None:
            with open(pca_path, "rb") as f:
                assets = pickle.load(f)
                scaler = assets['scaler']
                pca = assets['pca']

            X_val = np.stack(df_val['image_embedding'].values)
            X_val_pca = pca.transform(scaler.transform(X_val))
            df_val['embedding_pca'] = list(X_val_pca.astype(np.float16))

            df_val.drop(columns=['image_embedding']).to_pickle(
                "baseline_assets/val_test_data.pkl"
            )

            print("âœ… Val æ•°æ®å¤„ç†å®Œæˆ")
    else:
        print("âŒ æœªæ‰¾åˆ° baseline èµ„äº§ï¼Œè¯·å…ˆç”Ÿæˆè®­ç»ƒé›†åŸºå‡†")
